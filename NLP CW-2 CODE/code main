pip install pandas numpy torch transformers scikit-learn
# data_preprocessing.py
import pandas as pd
from sklearn.model_selection import train_test_split

def load_data(train_path, test_path):
    train_df = pd.read_csv(train_path, sep='\t')
    test_df = pd.read_csv(test_path, sep='\t')

    return train_df, test_df

train_path = '/content/lcp_single_train.tsv'
test_path = '/content/lcp_single_test.tsv'

train_df, test_df = load_data(train_path, test_path)

print(train_df.head())
from transformers import BertTokenizer, BertModel
import torch
import numpy as np

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
bert_model = BertModel.from_pretrained("bert-base-uncased")

def get_bert_embedding(sentence):
    tokens = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True)
    with torch.no_grad():
        output = bert_model(**tokens)
    # Average pooling of last hidden states
    return output.last_hidden_state.mean(dim=1).squeeze().numpy()

# Get embeddings for train and test
X_train = np.vstack(train_df['sentence'].apply(get_bert_embedding))
X_test = np.vstack(test_df['sentence'].apply(get_bert_embedding))
y_train = train_df['complexity'].values
# model_fnn.py
import torch.nn as nn

class FNN(nn.Module):
    def __init__(self, input_dim):
        super(FNN, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return self.model(x)
# model_lstm.py
class LSTMRegressor(nn.Module):
    def __init__(self, hidden_dim=128):
        super(LSTMRegressor, self).__init__()
        self.embedding = nn.Embedding(30522, 300)  # use BERT vocab size
        self.lstm = nn.LSTM(300, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, input_ids):
        embeds = self.embedding(input_ids)
        _, (hn, _) = self.lstm(embeds)
        return self.fc(hn[-1])
# model_cnn.py
class CNNRegressor(nn.Module):
    def __init__(self, vocab_size=30522, embed_dim=300):
        super(CNNRegressor, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.conv = nn.Conv1d(embed_dim, 128, kernel_size=5)
        self.pool = nn.AdaptiveMaxPool1d(1)
        self.fc = nn.Linear(128, 1)

    def forward(self, input_ids):
        x = self.embedding(input_ids).permute(0, 2, 1)  # (batch, embed_dim, seq_len)
        x = self.pool(torch.relu(self.conv(x))).squeeze(2)
        return self.fc(x)from torch.utils.data import DataLoader, TensorDataset
import torch.optim as optim
import torch.nn as nn

def train_model(model, X_train, y_train, epochs=5, lr=1e-4):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)

    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1).to(device)

    dataset = TensorDataset(X_train_tensor, y_train_tensor)
    dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        epoch_loss = 0
        for X_batch, y_batch in dataloader:
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        print(f"Epoch {epoch+1}: Loss = {epoch_loss/len(dataloader):.4f}")
# main.py
import torch.nn as nn

class FNN(nn.Module):
    def __init__(self, input_dim):
        super(FNN, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 1)
        )

    def forward(self, x):
        return self.model(x)

model = FNN(input_dim=X_train.shape[1])
train_model(model, X_train, y_train)
from transformers import BertTokenizer
import torch

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

def tokenize_sentences(sentences, max_length=50):
    tokens = tokenizer(sentences.tolist(), truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')
    return tokens['input_ids']

X_train_ids = tokenize_sentences(train_df['sentence'])
X_test_ids = tokenize_sentences(test_df['sentence'])
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# LSTM Regressor
class LSTMRegressor(nn.Module):
    def __init__(self, vocab_size=30522, embedding_dim=300, hidden_dim=128):
        super(LSTMRegressor, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, input_ids):
        x = self.embedding(input_ids)
        _, (hidden, _) = self.lstm(x)
        return self.fc(hidden[-1])

# Training function
def train_sequence_model(model, X_train_ids, y_train_tensor, epochs=5, lr=1e-4):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
dataset = TensorDataset(X_train_ids, y_train_tensor)
    loader = DataLoader(dataset, batch_size=32, shuffle=True)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"[LSTM] Epoch {epoch+1} | Loss: {total_loss/len(loader):.4f}")

# Run training
lstm_model = LSTMRegressor()
train_sequence_model(lstm_model, X_train_ids, y_train_tensor)
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

# CNN Regressor
class CNNRegressor(nn.Module):
    def __init__(self, vocab_size=30522, embedding_dim=300):
        super(CNNRegressor, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.conv = nn.Conv1d(in_channels=embedding_dim, out_channels=128, kernel_size=5)
        self.pool = nn.AdaptiveMaxPool1d(1)
        self.fc = nn.Linear(128, 1)

    def forward(self, input_ids):
        x = self.embedding(input_ids).permute(0, 2, 1)  # (batch, embed_dim, seq_len)
        x = torch.relu(self.conv(x))
        x = self.pool(x).squeeze(2)  # (batch, channels)
        return self.fc(x)

# Training function
def train_sequence_model(model, X_train_ids, y_train_tensor, epochs=5, lr=1e-4):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    
    dataset = TensorDataset(X_train_ids, y_train_tensor)
    loader = DataLoader(dataset, batch_size=32, shuffle=True)

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            output = model(X_batch)
            loss = criterion(output, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"[CNN] Epoch {epoch+1} | Loss: {total_loss/len(loader):.4f}")

# Run training
cnn_model = CNNRegressor()
train_sequence_model(cnn_model, X_train_ids, y_train_tensor)
from sklearn.model_selection import train_test_split

# Split original training data into 80% train, 20% validation
train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)

# Prepare validation inputs
X_val_embed = embed_sentences(val_df['sentence'])
X_val_ids = tokenize_sentences(val_df['sentence'])
y_val = val_df['complexity'].values
y_val_tensor = torch.tensor(y_val, dtype=torch.float32).unsqueeze(1)
from transformers import BertTokenizer, BertModel
import torch

# Load BERT tokenizer and model (if not done already)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
bert_model = BertModel.from_pretrained('bert-base-uncased')

# Function to embed sentences using BERT [CLS] token
def embed_sentences(sentences):
    bert_model.eval()
    embeddings = []
    with torch.no_grad():
        for sentence in sentences:
            inputs = tokenizer(sentence, return_tensors="pt", truncation=True, padding=True)
            outputs = bert_model(**inputs)
            cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token
            embeddings.append(cls_embedding.squeeze(0))
    return torch.stack(embeddings)
# Define the predict function
def predict(model, inputs, is_embed=False):
    """
    Gets predictions from a given model using provided inputs.

    Args:
        model: The trained PyTorch model.
        inputs: The input data for prediction.
        is_embed: If True, inputs are BERT embeddings; otherwise, token IDs.

    Returns:
        Numpy array of predictions.
    """
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():  # Disable gradient calculations
        if is_embed:
            inputs = inputs.to(device)  # Move embeddings to the correct device if necessary
        else:
            inputs = inputs.to(device)  # Move token IDs to the correct device
        outputs = model(inputs)  
    return outputs.cpu().numpy()  # Get predictions and move them to CPU

# Import device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# For FNN
fnn_model = model  # or whatever variable you used for FNN

# For LSTM and CNN (if needed)
# lstm_model = ...
# cnn_model = ...
fnn_model = model
lstm_model= model
cnn_model= model
def predict(model, X, is_embed=False):
    model.eval()
    with torch.no_grad():
        if is_embed:
            inputs = X.float()  # FNN: embeddings
        else:
            inputs = X.long()   # LSTM/CNN: token IDs
        outputs = model(inputs)
        return outputs.squeeze().numpy()
class LSTMRegressor(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=64):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = self.embedding(x)         # Token IDs → Embeddings
        _, (hn, _) = self.lstm(x)     # Get hidden state
        out = self.fc(hn[-1])         # Final prediction
        return out
vocab_size = tokenizer.vocab_size
lstm_model = LSTMRegressor(vocab_size)
import torch.nn as nn

class LSTMRegressor(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=64):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = self.embedding(x)            # [batch, seq] → [batch, seq, embed_dim]
        _, (hn, _) = self.lstm(x)        # hn: [1, batch, hidden_dim]
        out = self.fc(hn[-1])            # [batch, 1]
        return out
class CNNRegressor(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, num_filters=100, kernel_size=3):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.conv = nn.Conv1d(embed_dim, num_filters, kernel_size)
        self.relu = nn.ReLU()
        self.pool = nn.AdaptiveMaxPool1d(1)
        self.fc = nn.Linear(num_filters, 1)

    def forward(self, x):
        x = self.embedding(x)            # [batch, seq, embed_dim]
        x = x.permute(0, 2, 1)           # [batch, embed_dim, seq]
        x = self.relu(self.conv(x))      # [batch, num_filters, L_out]
        x = self.pool(x).squeeze(-1)     # [batch, num_filters]
        out = self.fc(x)                 # [batch, 1]
        return out
vocab_size = tokenizer.vocab_size

# Re-initialize and train LSTM
lstm_model = LSTMRegressor(vocab_size)
train_sequence_model(lstm_model, X_train_ids, y_train_tensor)

# Re-initialize and train CNN
cnn_model = CNNRegressor(vocab_size)
train_sequence_model(cnn_model, X_train_ids, y_train_tensor)
def predict(model, X, is_embed=False):
    model.eval()
    with torch.no_grad():
        if is_embed:
            inputs = X.float()
        else:
            inputs = X.long()
        outputs = model(inputs)
        return outputs.squeeze().numpy()
y_pred_fnn = predict(fnn_model, X_val_embed, is_embed=True)
y_pred_lstm = predict(lstm_model, X_val_ids)
y_pred_cnn = predict(cnn_model, X_val_ids)
from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

def evaluate_predictions(y_true, y_pred, model_name="Model"):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    print(f"[{model_name}] RMSE: {rmse:.4f} | MAE: {mae:.4f}")
    return rmse, mae
# FNN
evaluate_predictions(y_val_tensor.numpy(), y_pred_fnn, "FNN")

# LSTM
evaluate_predictions(y_val_tensor.numpy(), y_pred_lstm, "LSTM")

# CNN
evaluate_predictions(y_val_tensor.numpy(), y_pred_cnn, "CNN")
import matplotlib.pyplot as plt

def plot_preds(y_true, y_pred, model_name="Model"):
    plt.figure(figsize=(6, 4))
    plt.scatter(y_true, y_pred, alpha=0.6, edgecolor='k')
    plt.plot([0, 1], [0, 1], 'r--')
    plt.title(f"{model_name} — Predicted vs Actual")
    plt.xlabel("Actual Complexity")
    plt.ylabel("Predicted Complexity")
    plt.grid(True)
    plt.tight_layout()
    plt.show()

plot_preds(y_val_tensor.numpy(), y_pred_fnn, "FNN")
plot_preds(y_val_tensor.numpy(), y_pred_lstm, "LSTM")
plot_preds(y_val_tensor.numpy(), y_pred_cnn, "CNN")

# For CNN, use tokenized input
X_test_ids = tokenize_sentences(test_df['sentence'])

# Predict using CNN
y_test_pred = predict(cnn_model, X_test_ids)
submission_df = test_df.copy()
submission_df['predicted_complexity'] = y_test_pred

# Save to file
submission_df[['id', 'predicted_complexity']].to_csv("cnn_predictions.csv", index=False)
print("✅ Test set predictions saved to cnn_predictions.csv")
submission_df = test_df.copy()
submission_df['predicted_complexity'] = y_test_pred

# Save to file
submission_df[['id', 'predicted_complexity']].to_csv("lstm_predictions.csv", index=False)
print("✅ Test set predictions saved to lstm_predictions.csv")
submission_df = test_df.copy()
submission_df['predicted_complexity'] = y_test_pred

# Save to file
submission_df[['id', 'predicted_complexity']].to_csv("fnn_predictions.csv", index=False)
print("✅ Test set predictions saved to fnn_predictions.csv")

